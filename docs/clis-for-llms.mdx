---
description: "Generate structured documentation that LLMs can consume for tool use"
---

# CLIs for LLMs

argsh can generate structured output that Large Language Models (LLMs) can consume to understand and invoke your CLI tool. This enables AI agents to discover commands, flags, and types without manual prompt engineering.

## The Problem

LLMs need structured, unambiguous descriptions of CLI tools to use them effectively. Manual documentation drifts from the actual implementation. argsh solves this by generating machine-readable descriptions directly from your `:usage` and `:args` declarations.

## YAML for Tool Use

The `docgen yaml` output is the most useful format for LLMs:

```bash
./myapp docgen yaml
```

This produces structured data that maps directly to tool-use schemas:

```yaml
name: "myapp"
description: "My application server"
synopsis: "myapp [command] [options]"
commands:
  - name: "serve"
    description: "Start the server"
  - name: "build"
    description: "Build the project"
options:
  - name: "verbose"
    short: "v"
    description: "Enable verbose output"
    type: boolean
  - name: "config"
    short: "c"
    description: "Config file path"
    type: "string"
  - name: "help"
    short: "h"
    description: "Show this help message"
    type: boolean
```

## Converting to Tool Schemas

### OpenAI Function Calling

Transform the YAML output into an OpenAI function definition:

```python
import yaml, json

# Generate from your CLI
# ./myapp docgen yaml > cli.yaml

with open("cli.yaml") as f:
    cli = yaml.safe_load(f)

tools = []
for cmd in cli["commands"]:
    properties = {}
    for opt in cli["options"]:
        prop = {"description": opt["description"]}
        if opt["type"] == "boolean":
            prop["type"] = "boolean"
        else:
            prop["type"] = "string"
        properties[opt["name"]] = prop

    tools.append({
        "type": "function",
        "function": {
            "name": f"{cli['name']}_{cmd['name']}",
            "description": cmd["description"],
            "parameters": {
                "type": "object",
                "properties": properties,
            }
        }
    })
```

### Anthropic Tool Use

```python
tools = []
for cmd in cli["commands"]:
    input_schema = {"type": "object", "properties": {}}
    for opt in cli["options"]:
        input_schema["properties"][opt["name"]] = {
            "description": opt["description"],
            "type": "boolean" if opt["type"] == "boolean" else "string",
        }

    tools.append({
        "name": f"{cli['name']}_{cmd['name']}",
        "description": cmd["description"],
        "input_schema": input_schema,
    })
```

## Markdown for Context Windows

For simpler use cases — like pasting CLI documentation into a system prompt — use the Markdown output:

```bash
./myapp docgen md
```

This gives LLMs a human-readable reference that works well as context:

```
You have access to the myapp CLI tool.

$(./myapp docgen md)

Use this tool by running shell commands.
```

## Automation

Generate LLM-ready documentation in CI:

```yaml
steps:
  - name: Generate LLM tool definitions
    run: |
      ./myapp docgen yaml > tools/myapp.yaml
      python scripts/yaml_to_tools.py tools/myapp.yaml > tools/myapp_tools.json
```

Since the documentation is generated from code, it stays in sync automatically. Every release includes accurate tool definitions without manual updates.
